Software testing plays an important role in a project since it helps developers to gain confidence that software works as expected \cite{Pressman:2007}. Moreover, testing is fundamental for reducing risks and assessing software quality \cite{Pressman:2007}.


On the other hand, testing activities are known to be complex and costly. Studies have found that nearly 50\% of a project's budget is related to testing \cite{kumar2016impacts}. Moreover, in practice, a test suite can combine manually and automatically executed test cases \cite{manualtesting:2009:Itkonen}. Although automation is desired, manually executed test cases are still very important. Itkonen et al. \cite{manualtesting:2009:Itkonen} state that manual testing plays an important role in the software industry and cannot be fully replaced by automatic testing. For instance, a tester that runs manual tests tents to better exercise a GUI and find new faults.

In this sense, several works guide efforts on trying to reduce the costs of testing. For instance, Model-Based Testing (MBT) \cite{dalal:mbt:1999,Utting:2006:PMT:1200168} is a strategy where test suites are automatically generated from specification models (e.g., use cases, UML diagrams). By using MBT, sound tests are extracted before any coding, and without much effort.

In the context of agile methodologies (e.g., eXtreme Programming \cite{beck2000extremeprogramming} and Scrum \cite{sutherland2014scrum}), where requirement changes are frequent to cope with client demands, test suites are used as safety nets for avoiding feature regression. In this scenario, an always updated test suite is mandatory. A recent work has proposed lightweight specification artifacts for enabling the use of MBT in agile projects \cite{dalton2018mbtagile}, CLARET. With CLARET one can both specify requirements using use cases and generate MBT suites from them.  

However, a different problem has emerged. As software evolves (e.g., bug fixes, change requirements, refactorings), both its models and test suite need revisions. In practice, since MBT test suites are generated from requirement models, as requirements change, the requirement artifacts are updated, and new test suites are generated. The new suites replace the old ones. Therefore, test cases that were impacted by the edits, instead of updated, are discarded \cite{de2016full}. 
%Silva et al. \cite{Silva:2018:SIM:3266003.3266009}
Silva et al. \cite{Silva:2018:SIM:3266003.3266009} ran a study with MBT test suites which found that 86\% of the test cases turn obsolete between two consecutive versions of a requirement file, and therefore are discarded. An obsolete test case is a test that includes at least one step that differs from the updated version of the specification document. Although one may find easy to just generate new tests, regression testing is based on a stable test suite that evolves. Test case discarding implies on important history data that are missed (e.g., execution time, the link faults-to-tests, fault-discovering time). In a scenario where development are guided by previously detected faults, missing tests can be a huge loss. Moreover, discard and poor testing are known to be signs of bad management and eventually lead to software development waste \cite{sedano2017waste}.

%By discarding a test case, important history data are lost (e.g., test execution history). 

Silva et al. \cite{Silva:2018:SIM:3266003.3266009} also mention 
that part of the obsolete test cases could be easily reused with little effort and consequently reducing testing discard. However, manual analysis is tedious, costly and time-consuming, which often prevents its applicability in the agile context.  In this sense, there is a need for an automatic way of detecting reusable test cases. 

Distance functions \cite{cohen2003distance} map a pair of strings to a real number that indicates the similarity level between the two versions. In a scenario where manual test cases evolve due to requirement changes, distance functions can be an interesting tool to help us classify the impact of the changes into a test case.  

In this paper, we report a series of empirical studies using real industrial projects aiming at analyzing the efficiency of using distance functions to reclassify reusable test cases from the obsolete set. 
%\hl{In the context of this study, we consider impacted test cases as any test case that was affected by a system specification evolution.}
We mined the evolution of two projects that use MBT suites, and we use ten distance functions to classify edits between \textit{low impact} (e.g., rewording, typo fixing) --require little test case updating-- and \textit{high impact} (specification and functionality changes) --require much test updating. Our results have found that all ten functions perform well on classifying edit impact. Moreover, we found the optimal configuration for using each function. Finally, we ran a case study in which our strategy was able to reduce the test case discard by at least 15\%.

This paper is organized as follows. In section \ref{sec:motiv}, we present a motivational example. The needed background is discussed in Section \ref{sec:background}. Sections \ref{sec:es} and \ref{sec:case} present the performed empirical study and case study, respectively. In Section \ref{sec:threats}, some threats to validity are cleared. Finally, Sections \ref{sec:related} and \ref{sec:conclud} present related works and the concluding remarks.