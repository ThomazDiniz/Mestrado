Most of the threats for validity to the drew conclusions refer to the number of projects, use cases, and test cases used in our empirical studies. Those numbers were limited to the artifacts created in the context of the selected projects. Therefore, our results cannot be generalized beyond the three projects (SAFF, BZC, and TCOM). However, it is important to highlight that all used artifacts are from real industrial systems from different contexts. 

As for conclusion validity, our studies deal with
a limited data set. Again, since we chose to work with real, instead of artificial artifacts, the data available for analysis were limited. However, the data was validated by the team engineers
and by the authors. 

One may argue that since our study deals only with CLARET use cases and test cases, our results are not valid for other notations. However, CLARET resembles traditional specification formats (e.g., UML Use Cases). Moreover, CLARET test cases are basically a sequence of pairs of steps (user input - system response), which can relate to most manual testing at the system level.

Regarding internal validity, we collected the changed set from the project's repositories, and we manually classify each change according to its impact. This manual validation was performed by at least two of the authors and, when needed, the projectâ€™s members were consulted. Moreover, we reused open-source implementations of the distance functions\footnote{https://github.com/luozhouyang/python-string-similarity}. These implementations were also validated by the first author.