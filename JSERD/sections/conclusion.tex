In this paper, we describe a series of empirical studies ran on industrial systems for evaluating the use of distance functions %for automatically classify the impact of edits in use case files. 
to classify the impact of edits in use case files automatically. Our results showed that distance functions are effective in identifying low impact editions. Therefore, we propose two variations of its use: as a classification strategy itself (Section \ref{sec:es}), and combined with a machine learning model (Section \ref{sec:ml}).

We also found that low impact editions often refer to test cases that can be easily updated without any effort. Our strategies helped to both identify low impact and high impact test cases. We believe those results can help testers to better work with MBT artifacts in the context of software evolution and avoid the discard of test cases.

%As future work we plan to expand our study with a broader set of systems; to develop a tool that, using distance functions, can help the tester to identify and update low impact test cases; and investigate the use of different approaches (e.g., machine learning) to help testers to ways update highly impacted test cases and reuse history data of obsolete MBT test cases.
As future work, we plan to expand our study with a broader set of systems. We also consider developing a tool that, using distance functions, can help testers to identify and update low impact test cases. Finally, we plan to investigate the use of different approaches (e.g., other machine learning techniques, dictionaries) to improve our classification rates and better help testers when updating highly impacted test cases.