In previous sections, we proposed two different strategies for predicting the impact of model edits and avoiding test case discarding: (i) a pure distance function-based; and (ii) a strategy that combines machine learning with distance values. Both were evaluated in a case study with real data. The first strategy applies a simpler analysis, which may infer lower costs. Though simple, it was able to correctly identify 63\% of the \textit{low impacted} test cases, and to rescue 9.53\% of the test cases that would be discarded. However, it did not perform well when classifying \textit{highly impacted} tests (19\%). Our second approach, though more complex (it requires a set of distance values as inputs to the model), generated better results for classifying \textit{low impacted} and \textit{highly impacted} test cases, 68\%, and 86\% precision, respectively. Moreover, it helped us to avoid the discard of 10.4\% of the test cases. Therefore, if running several distance functions for each model edit is not an issue, we recommend the use of (ii) since it is in fact the best option for automatically classify test cases that should be reused (\textit{low impacted}) or be discarded (highly impacted). Moreover, regarding time, our prediction model responses were almost instant. Regarding \textit{mixed} tests, our suggestion is always to inspect them to decide whether it is worth updating.